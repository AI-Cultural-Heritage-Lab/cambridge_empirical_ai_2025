{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# LLM Data Processing Pipeline\n",
    "\n",
    "This notebook processes the LLM sanitization dataset by cleaning, restructuring, and transforming responses from multiple sources (USHMM, GPT-4o, Gemini, and Grok) into a standardized format suitable for analysis.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The pipeline transforms the original wide-format dataset into a long-format structured dataset where each row represents one response from one source to one query. This allows for systematic comparison across different LLM sources and enables various types of analysis on response quality, language detection, and content processing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# 1. Setup: Imports and OpenAI Configuration\n",
    "\n",
    "Loads core libraries for text processing and initializes the OpenAI client for structured GPT-4o interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== IMPORTS =====\n",
    "import os                          # Securely fetches environment variables (API key)\n",
    "import re                          # Regex‑based text cleansing helpers\n",
    "import glob                        # Batch file discovery for \\*.csv, \\*.json, etc.\n",
    "from typing import Any             # Static typing for helper stubs\n",
    "\n",
    "import pandas as pd                # DataFrame operations and I/O\n",
    "from langdetect import detect      # Lightweight language ID (<20 KB model)\n",
    "from openai import OpenAI          # Official OpenAI Python SDK\n",
    "from pydantic import BaseModel, Field  # Data‑validation & parsing\n",
    "\n",
    "# ===== CONFIGURATION =====\n",
    "# OpenAI API Configuration\n",
    "openai_api_key = 'Your Key'\n",
    "client = OpenAI(api_key=openai_api_key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Loading and Initial Examination\n",
    "\n",
    "Loads the original CSV dataset and inspects its structure, columns, and sample rows to understand the data format and content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (1000, 5)\n",
      "Columns in the dataset:\n",
      "['original_query', 'ushmm_article', 'chatgpt_4o_response', 'gemini_response', 'grok_response']\n",
      "\n",
      "First few rows:\n",
      "                          original_query  \\\n",
      "0  how many people died in the holocaust   \n",
      "1                      armenian genocide   \n",
      "2                 holocaust encyclopedia   \n",
      "3                    first they came for   \n",
      "4                              holocaust   \n",
      "\n",
      "                                       ushmm_article  \\\n",
      "0  #How Many People did the Nazis Murder? | Holoc...   \n",
      "1  #The Armenian Genocide (1915-16): Overview | H...   \n",
      "2  #Introduction to the Holocaust: What was the H...   \n",
      "3  #Martin Niemöller: \"First they came for the So...   \n",
      "4  #Introduction to the Holocaust: What was the H...   \n",
      "\n",
      "                                 chatgpt_4o_response  \\\n",
      "0  Approximately 6 million Jews were killed durin...   \n",
      "1  The Armenian Genocide was the systematic mass ...   \n",
      "2  The Holocaust Encyclopedia is a comprehensive ...   \n",
      "3  The phrase \"First they came for...\" is the ope...   \n",
      "4  The Holocaust was the systematic, state-sponso...   \n",
      "\n",
      "                                     gemini_response  \\\n",
      "0  Historians estimate that the Nazis murdered ap...   \n",
      "1  The Armenian Genocide was the systematic destr...   \n",
      "2  The Holocaust Encyclopedia is a comprehensive ...   \n",
      "3  \"First they came for the socialists, and I did...   \n",
      "4  The Holocaust was the systematic, state-sponso...   \n",
      "\n",
      "                                       grok_response  \n",
      "0  The Holocaust was a period of systematic perse...  \n",
      "1  The Armenian Genocide was the systematic exter...  \n",
      "2  The Holocaust Encyclopedia is a comprehensive ...  \n",
      "3  \"First they came ...\" is the beginning of a fa...  \n",
      "4  Der Holocaust war eine systematische, staatlic...  \n"
     ]
    }
   ],
   "source": [
    "# ===== DATA LOADING =====\n",
    "# Load the original dataset and examine structure\n",
    "df = pd.read_csv('../original_data_1000_queries.csv')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(\"Columns in the dataset:\")\n",
    "print(df.columns.tolist())\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Text Cleaning Helper Functions\n",
    "\n",
    "Defines comprehensive text cleaning functions to remove unwanted elements from articles, especially USHMM encyclopedia entries which contain extensive metadata and formatting. These functions systematically clean text by removing various unwanted elements\n",
    "\n",
    "## Footnote & Section Removal\n",
    "- **`remove_double_numbered_footnotes(text)`** – Removes double-numbered footnotes and their headers.\n",
    "- **`remove_critical_thinking_block(text)`** – Removes “Critical Thinking Questions” sections and their bullet points.\n",
    "- **`remove_language_section(text)`** – Removes multilingual availability notices and associated bullets.\n",
    "- **`remove_tags_section(text)`** – Removes “Tags” sections including headers and bullet items.\n",
    "- **`remove_unique_tags_and_footers(text)`** – Removes USHMM footers, branding tags, and extra separators.\n",
    "\n",
    "## General Markdown and Refusal Cleaning\n",
    "- **`remove_headers(md)`** – Removes markdown headers, standalone bold lines, and Setext-style titles.\n",
    "- **`check_for_refusal(response)`** – Returns `'yes'` if refusal language like “sorry” or “AI assistant” is found.\n",
    "- **`remove_all_markdown_keep_table_content(md)`** – Strips markdown syntax while preserving table content.\n",
    "\n",
    "## Comprehensive USHMM Article Cleaner\n",
    "- **`clean_ushmm_article(article)`** – Applies all relevant filters and cleanup functions to USHMM articles.\n",
    "\n",
    "\n",
    "### USHMM-Specific Cleaning:\n",
    "USHMM articles contain extensive metadata that needs removal:\n",
    "- Author attribution lines\n",
    "- Museum attribution\n",
    "- Citation/print/share buttons\n",
    "- Image captions and credits\n",
    "- Video player fallback text\n",
    "- Last edited timestamps\n",
    "- Source attributions\n",
    "\n",
    "### Text Processing Strategy:\n",
    "1. **Line-by-line filtering**: Removes specific unwanted line patterns\n",
    "2. **Section removal**: Identifies and removes entire sections (footnotes, questions, etc.)\n",
    "3. **Format normalization**: Standardizes whitespace and removes multiple consecutive newlines\n",
    "4. **Content preservation**: Ensures core article text remains intact and readable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== HELPER FUNCTIONS =====\n",
    "\n",
    "# --- Footnote & Section Removal ---\n",
    "\n",
    "def remove_double_numbered_footnotes(text):\n",
    "    \"\"\"Remove footnotes sections with double numbering format\"\"\"\n",
    "    # First remove the ## Footnotes header and following blank line\n",
    "    text = re.sub(r'## Footnotes\\n\\n', '', text)\n",
    "    # Then remove the double numbered footnotes and their content\n",
    "    text = re.sub(\n",
    "        r'\\d+\\.\\s+\\d+\\.\\s*\\n(?:\\s{2,}.*\\n?)+',\n",
    "        '',\n",
    "        text\n",
    "    )\n",
    "    return text\n",
    "\n",
    "def remove_critical_thinking_block(text):\n",
    "    \"\"\"Remove Critical Thinking Questions sections\"\"\"\n",
    "    lines = text.splitlines()\n",
    "    result = []\n",
    "    inside_ctq_block = False\n",
    "    \n",
    "    for line in lines:\n",
    "        if line.strip() == \"## Critical Thinking Questions\":\n",
    "            inside_ctq_block = True\n",
    "            continue  # skip the header line\n",
    "        \n",
    "        if inside_ctq_block:\n",
    "            # Skip the blank line after the header\n",
    "            if line.strip() == \"\":\n",
    "                continue\n",
    "            # Skip all bullet points that start with *\n",
    "            elif line.strip().startswith(\"*\"):\n",
    "                continue\n",
    "            # If we encounter another section header (##), we're out of CTQ block\n",
    "            elif line.strip().startswith(\"##\"):\n",
    "                inside_ctq_block = False\n",
    "                result.append(line)\n",
    "            # If we encounter any other non-empty content that's not a bullet point,\n",
    "            # we're likely out of the CTQ section\n",
    "            elif line.strip() and not line.strip().startswith(\"*\"):\n",
    "                inside_ctq_block = False\n",
    "                result.append(line)\n",
    "            # Continue skipping if it's still part of CTQ block\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            result.append(line)\n",
    "    \n",
    "    return \"\\n\".join(result)\n",
    "\n",
    "def remove_language_section(text):\n",
    "    \"\"\"Remove language availability sections\"\"\"\n",
    "    lines = text.splitlines()\n",
    "    cleaned_lines = []\n",
    "    skip = False\n",
    "    expecting_bullets = False\n",
    "\n",
    "    for line in lines:\n",
    "        stripped = line.strip()\n",
    "\n",
    "        # Detect header line\n",
    "        if \"### This content is available in the following languages\" in stripped:\n",
    "            skip = True\n",
    "            expecting_bullets = True\n",
    "            continue\n",
    "\n",
    "        if skip:\n",
    "            # Handle blank lines after header\n",
    "            if expecting_bullets and stripped == \"\":\n",
    "                continue\n",
    "\n",
    "            # Handle bullet lines\n",
    "            if stripped.startswith(\"*\") or stripped.startswith(\"+\"):\n",
    "                expecting_bullets = False  # We found bullets, no longer expecting them\n",
    "                continue\n",
    "            else:\n",
    "                # Found a non-bullet line → stop skipping\n",
    "                skip = False\n",
    "\n",
    "        if not skip:\n",
    "            cleaned_lines.append(line)\n",
    "\n",
    "    return \"\\n\".join(cleaned_lines)\n",
    "\n",
    "def remove_tags_section(text):\n",
    "    \"\"\"Remove tags sections from articles\"\"\"\n",
    "    lines = text.splitlines()\n",
    "    cleaned_lines = []\n",
    "    skip = False\n",
    "    expecting_bullets = False\n",
    "\n",
    "    for line in lines:\n",
    "        stripped = line.strip()\n",
    "\n",
    "        # Detect both header types\n",
    "        if stripped == \"#### Tags\" or stripped == \"Tags\":\n",
    "            skip = True\n",
    "            expecting_bullets = True\n",
    "            continue\n",
    "\n",
    "        if skip:\n",
    "            if expecting_bullets:\n",
    "                if stripped == \"\":\n",
    "                    continue  # skip the blank line right after the header\n",
    "                elif stripped.startswith((\"*\", \"+\", \"-\")):\n",
    "                    expecting_bullets = False\n",
    "                    continue\n",
    "                else:\n",
    "                    skip = False  # Unexpected format, stop skipping\n",
    "            else:\n",
    "                if stripped.startswith((\"*\", \"+\", \"-\")):\n",
    "                    continue  # keep skipping bullet lines\n",
    "                else:\n",
    "                    skip = False  # End of tag block\n",
    "\n",
    "        if not skip:\n",
    "            cleaned_lines.append(line)\n",
    "\n",
    "    return \"\\n\".join(cleaned_lines)\n",
    "\n",
    "\n",
    "def remove_unique_tags_and_footers(text: str) -> str:\n",
    "    \"\"\"Remove unique tags, footers, single-dash lines, and collapse blank lines.\"\"\"\n",
    "    # 1) Remove sections from '--- ### Tags' up to the next '---'\n",
    "    text = re.sub(\n",
    "        r'---\\s*### Tags(?:\\n.+?)*?(?=\\n---)', \n",
    "        '', \n",
    "        text, \n",
    "        flags=re.DOTALL\n",
    "    )\n",
    "    # 2) Drop the USHMM footer line\n",
    "    text = re.sub(r'\\* US Holocaust Memorial Museum', '', text)\n",
    "    # 3) Normalize lines consisting only of '---' into a single newline\n",
    "    text = re.sub(r'(?m)^[ \\t]*---[ \\t]*\\n?', '\\n', text)\n",
    "    # 4) Remove only lines that consist of a single dash\n",
    "    lines = text.splitlines()\n",
    "    kept = [\n",
    "        line\n",
    "        for line in lines\n",
    "        if line.strip() != '-'\n",
    "    ]\n",
    "    # 5) Rejoin and collapse 3+ newlines into 2\n",
    "    cleaned = \"\\n\".join(kept)\n",
    "    cleaned = re.sub(r'\\n{3,}', '\\n\\n', cleaned)\n",
    "    return cleaned.strip()\n",
    "\n",
    "\n",
    "\n",
    "# --- Markdown and Refusal Cleaning ---\n",
    "\n",
    "def remove_headers(md: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove all Markdown headers (lines starting with '#'), standalone bullet/hyphen lines,\n",
    "    Setext-style headers, and lines containing only bold text (e.g., **hello**).\n",
    "    \"\"\"\n",
    "    lines = md.splitlines()\n",
    "    output_lines = []\n",
    "    skip_next = False\n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        stripped = line.strip()\n",
    "\n",
    "        if skip_next:\n",
    "            skip_next = False\n",
    "            continue\n",
    "\n",
    "        # Skip ATX-style headers (lines starting with '#')\n",
    "        if stripped.startswith('#'):\n",
    "            continue\n",
    "\n",
    "        # Skip standalone hyphens or asterisks\n",
    "        if stripped in ('-', '*'):\n",
    "            continue\n",
    "\n",
    "        # Skip lines that consist solely of bold text (e.g., **hello**)\n",
    "        if re.fullmatch(r'\\*\\*[^*]+\\*\\*', stripped):\n",
    "            continue\n",
    "\n",
    "        # Skip Setext-style headers (underlines of '=' or '-')\n",
    "        if i + 1 < len(lines) and re.fullmatch(r'[=-]+', lines[i + 1].strip()):\n",
    "            skip_next = True\n",
    "            continue\n",
    "\n",
    "        output_lines.append(line)\n",
    "    \n",
    "    # Collapse multiple newlines\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "\n",
    "    return \"\\n\".join(output_lines)\n",
    "\n",
    "def remove_all_markdown_keep_table_content(md: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove all common Markdown syntax from the input string,\n",
    "    while preserving the text content of tables.\n",
    "    \"\"\"\n",
    "    text = md\n",
    "\n",
    "    # Remove code blocks\n",
    "    text = re.sub(r'```[\\s\\S]*?```', '', text)\n",
    "\n",
    "    # Remove inline code\n",
    "    text = re.sub(r'`([^`]*)`', r'\\1', text)\n",
    "\n",
    "    # Remove images ![alt](url)\n",
    "    text = re.sub(r'!\\[.*?\\]\\(.*?\\)', '', text)\n",
    "\n",
    "    # Remove links [text](url)\n",
    "    text = re.sub(r'\\[([^\\]]+)\\]\\(.*?\\)', r'\\1', text)\n",
    "\n",
    "    # Remove bold and italic markers\n",
    "    text = re.sub(r'(\\*\\*|__)(.*?)\\1', r'\\2', text)\n",
    "    text = re.sub(r'(\\*|_)(.*?)\\1', r'\\2', text)\n",
    "\n",
    "    # Remove ATX headers\n",
    "    text = re.sub(r'^\\s{0,3}#{1,6}\\s*', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove Setext-style headers\n",
    "    text = re.sub(r'^[=\\-]{2,}\\s*$', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove blockquotes\n",
    "    text = re.sub(r'^\\s{0,3}>\\s?', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove list markers\n",
    "    text = re.sub(r'^\\s*[-*+]\\s+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'^\\s*\\d+\\.\\s+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove Markdown table formatting but keep content\n",
    "    text = re.sub(r'^\\s*\\|?[-:| ]+\\|?\\s*$', '', text, flags=re.MULTILINE)  # strip table rule lines\n",
    "    text = re.sub(r'^\\s*\\|\\s*', '', text, flags=re.MULTILINE)              # strip leading pipes\n",
    "    text = re.sub(r'\\s*\\|\\s*', '\\t', text)                                 # convert pipes to tabs\n",
    "    text = re.sub(r'\\|\\s*$', '', text, flags=re.MULTILINE)                 # remove trailing pipe\n",
    "\n",
    "    # Collapse multiple newlines\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "def check_for_refusal(response: str) -> str:\n",
    "    \"\"\"\n",
    "    Checks if a response contains refusal-like language.\n",
    "\n",
    "    Parameters:\n",
    "        response (str): The text response to analyze.\n",
    "\n",
    "    Returns:\n",
    "        str: 'yes' if refusal keywords are detected, 'no' otherwise.\n",
    "    \"\"\"\n",
    "    refusal_keywords = ['sorry', \"i cannot\", \"i'm sorry\", 'ai assistant']\n",
    "    response_lower = response.lower()\n",
    "    return 'yes' if any(keyword in response_lower for keyword in refusal_keywords) else 'no'\n",
    "\n",
    "# --- USHMM Article Cleaner ---\n",
    "\n",
    "def clean_ushmm_article(article):\n",
    "    \"\"\"Apply comprehensive cleaning to USHMM articles\"\"\"\n",
    "    # Skip if article contains error indicators\n",
    "    if any(x in article for x in ['Error', 'sorry']):\n",
    "        return article\n",
    "    \n",
    "    # Apply all cleaning steps for USHMM articles\n",
    "    if any(x in article for x in ['Author(s):', 'United States Holocaust Memorial Museum, Washington, DC', \n",
    "                                  '* US Holocaust Memorial Museum Collection', 'View Archival Details', \n",
    "                                  '## Footnotes', '* Cite', '* Print', '* Share', 'Last Edited:', \n",
    "                                  'caption=', 'credit=', '## Critical Thinking Questions', 'Source:', \n",
    "                                  'Primary resources by the Jewish Partisan Educational Foundation', '[![', \n",
    "                                  '![', '](', 'Your browser does not support the video tag.', \n",
    "                                  \"## This content is available in the following languages\", \"Tags\"]):\n",
    "        \n",
    "        # Split into lines and filter out unwanted lines\n",
    "        lines = article.split('\\n')\n",
    "        cleaned_lines = []\n",
    "        skip_next = False\n",
    "        \n",
    "        for i, line in enumerate(lines):\n",
    "            if skip_next:\n",
    "                skip_next = False\n",
    "                continue\n",
    "                \n",
    "            # Skip various unwanted content\n",
    "            if any(pattern in line for pattern in ['caption=', 'credit=', '* Cite', '* Print', '* Share', \n",
    "                                                  'Last Edited:', 'Author(s):', \n",
    "                                                  'United States Holocaust Memorial Museum, Washington, DC',\n",
    "                                                  'Source:', 'Primary resources by the Jewish Partisan Educational Foundation',\n",
    "                                                  '[![', '![', '](', 'Your browser does not support the video tag.']):\n",
    "                if 'caption=' in line or 'credit=' in line:\n",
    "                    skip_next = True\n",
    "                continue\n",
    "                \n",
    "            cleaned_lines.append(line)\n",
    "        \n",
    "        # Rejoin the cleaned lines\n",
    "        cleaned_article = '\\n'.join(cleaned_lines)\n",
    "        \n",
    "        # Apply function-based cleaning\n",
    "        if \"## Critical Thinking Questions\" in cleaned_article:\n",
    "            cleaned_article = remove_critical_thinking_block(cleaned_article)\n",
    "        if \"## Footnotes\" in cleaned_article:\n",
    "            cleaned_article = remove_double_numbered_footnotes(cleaned_article)\n",
    "        if \"## This content is available in the following languages\" in cleaned_article:\n",
    "            cleaned_article = remove_language_section(cleaned_article)  \n",
    "        if \"Tags\" in cleaned_article:\n",
    "            cleaned_article = remove_tags_section(cleaned_article)\n",
    "        cleaned_article = remove_unique_tags_and_footers(cleaned_article)\n",
    "            \n",
    "        # Remove multiple consecutive newlines\n",
    "        cleaned_article = re.sub(r'\\n{3,}', '\\n', cleaned_article)\n",
    "        return cleaned_article.strip()\n",
    "    \n",
    "    return article.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# 4. Data Restructuring and Transformation\n",
    "\n",
    "## Purpose\n",
    "Transforms the original wide-format dataset into a structured long-format dataset following the specified schema, with each row representing one response from one source to one query.\n",
    "\n",
    "## What it does:\n",
    "\n",
    "### Schema Transformation:\n",
    "Converts from wide format (1 row = 1 query with 4 responses) to long format (4 rows = 1 query with 1 response each):\n",
    "\n",
    "**New Schema:**\n",
    "- **`id`**: Numeric unique query ID (1, 2, 3...)\n",
    "- **`original_query`**: The original question/prompt text\n",
    "- **`source`**: Response source (USHMM, gpt_4o, gemini, grok)\n",
    "- **`response`**: Original unprocessed response text\n",
    "- **`response_cleaned`**: Cleaned response (USHMM only - removes metadata)\n",
    "- **`response_no_headers`**: Response with markdown headers removed\n",
    "- **`response_no_headers_or_markdown`**: Response with all markdown formatting removed\n",
    "- **`response_language`**: Detected language code (en, es, de, etc.)\n",
    "- **`response_refusal`**: Yes/No flag for LLM refusal to answer\n",
    "- **`response_keep`**: Yes/No flag for if response should be kept (English, non-refusal)\n",
    "- **`response_already_complete_sentences`**: Yes/No flag for if original response (or cleaned response in case of USHMM) is already all complete sentences\n",
    "- **`response_complete_sentences`**: Text converted to complete sentences\n",
    "\n",
    "### Processing Pipeline per Response:\n",
    "1. **Language Detection**: Identifies response language using `langdetect`\n",
    "2. **Refusal Detection**: Checks for common refusal patterns (\"sorry\", \"I cannot\", etc.)\n",
    "3. **Source-Specific Cleaning**: Applies specialized cleaning for USHMM articles\n",
    "4. **Header Removal**: Strips markdown headers while preserving content\n",
    "5. **Markdown Removal**: Removes all formatting while keeping table content\n",
    "6. **Sentence Completion**: Converts to complete sentences (for quality responses only)\n",
    "\n",
    "### Quality Control:\n",
    "- **Conditional Processing**: Only processes English, non-refusal responses\n",
    "- **Error Handling**: Gracefully handles processing failures\n",
    "- **Length Validation**: Ensures reasonable text length after processing\n",
    "- **Preservation**: Maintains original text when processing isn't needed\n",
    "\n",
    "### USHMM Special Handling:\n",
    "USHMM articles receive comprehensive cleaning to remove:\n",
    "- Author attributions and museum credits\n",
    "- Navigation elements (Cite, Print, Share buttons)\n",
    "- Metadata (Last Edited, View Archival Details)\n",
    "- Media elements (images, videos, captions)\n",
    "- Footnotes and critical thinking questions\n",
    "- Language availability notices and tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== DATA RESTRUCTURING =====\n",
    "# Transform the dataset into the required structure with proper schema\n",
    "\n",
    "def create_base_dataframe(df):\n",
    "    \"\"\"\n",
    "    Transform the original dataframe into the base structured format with:\n",
    "    - id: numeric unique query ID\n",
    "    - original_query: string \n",
    "    - source: USHMM/gpt_4o/gemini/grok\n",
    "    - response: original response string\n",
    "    - response_cleaned: cleaned response (USHMM only)\n",
    "    \n",
    "    Other columns are initialized empty for later processing:\n",
    "    - response_no_headers\n",
    "    - response_no_headers_or_markdown\n",
    "    - response_language\n",
    "    - response_refusal\n",
    "    - response_keep\n",
    "    - response_keep_for_all_sources\n",
    "    - response_already_complete_sentences\n",
    "    - response_complete_sentences\n",
    "    \"\"\"\n",
    "    \n",
    "    structured_data = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        query_id = idx \n",
    "        original_query = row['original_query']\n",
    "        \n",
    "        # Process each source\n",
    "        sources = {\n",
    "            'USHMM': row['ushmm_article'],\n",
    "            'gpt_4o': row['chatgpt_4o_response'], \n",
    "            'gemini': row['gemini_response'],\n",
    "            'grok': row['grok_response']\n",
    "        }\n",
    "        \n",
    "        for source_name, response in sources.items():\n",
    "            if pd.isna(response):\n",
    "                continue\n",
    "                \n",
    "            # Only clean USHMM articles at this stage\n",
    "            response_cleaned = ''\n",
    "            if source_name == 'USHMM':\n",
    "                response_cleaned = clean_ushmm_article(str(response))\n",
    "            else:\n",
    "                response_cleaned = response\n",
    "            \n",
    "            # Create row with base fields populated and others empty\n",
    "            structured_row = {\n",
    "                'id': query_id,\n",
    "                'original_query': original_query,\n",
    "                'source': source_name,\n",
    "                'response': str(response),\n",
    "                'response_cleaned': response_cleaned,\n",
    "                'response_no_headers': '',\n",
    "                'response_no_headers_or_markdown': '',\n",
    "                'response_language': '',\n",
    "                'response_refusal': '',\n",
    "                'response_keep': '',\n",
    "                'response_keep_for_all_sources': '',\n",
    "                'response_already_complete_sentences': '',\n",
    "                'response_complete_sentences': ''\n",
    "            }\n",
    "            \n",
    "            structured_data.append(structured_row)\n",
    "    \n",
    "    return pd.DataFrame(structured_data)\n",
    "\n",
    "def process_responses(df):\n",
    "    \"\"\"Process all responses in the dataframe by cleaning text, removing headers/markdown,\n",
    "    detecting language, and checking for refusal patterns.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): The dataframe containing responses to process\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: DataFrame with all responses processed\n",
    "    \"\"\"\n",
    "    for idx, row in df.iterrows():\n",
    "        response_text = row['response_cleaned'] if row['source'] == 'USHMM' and row['response_cleaned'] else row['response']\n",
    "        \n",
    "        # Remove headers and markdown\n",
    "        no_headers = remove_headers(str(response_text))\n",
    "        no_headers = re.sub(r'\\n{3,}', '\\n\\n', no_headers)\n",
    "        df.at[idx, 'response_no_headers'] = no_headers\n",
    "        \n",
    "        no_markdown = remove_all_markdown_keep_table_content(no_headers)\n",
    "        no_markdown = re.sub(r'\\n{3,}', '\\n\\n', no_markdown)\n",
    "        df.at[idx, 'response_no_headers_or_markdown'] = no_markdown\n",
    "        \n",
    "        # Detect language\n",
    "        try:\n",
    "            lang = detect(no_markdown)\n",
    "            df.at[idx, 'response_language'] = lang\n",
    "        except:\n",
    "            df.at[idx, 'response_language'] = 'unknown'\n",
    "        \n",
    "        # Set refusal - 'no' for USHMM, check patterns for other sources\n",
    "        if row['source'] == 'USHMM':\n",
    "            if \"error: no main content found for \" in response_text.lower():\n",
    "                df.at[idx, 'response_refusal'] = 'yes'\n",
    "            else:\n",
    "                df.at[idx, 'response_refusal'] = 'no'\n",
    "        else:\n",
    "            df.at[idx, 'response_refusal'] = check_for_refusal(no_markdown)\n",
    "\n",
    "        # Set keep flag based on language and refusal status\n",
    "        df.at[idx, 'response_keep'] = 'yes' if (df.at[idx, 'response_language'] == 'en' and df.at[idx, 'response_refusal'] == 'no') else 'no'\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_keep_for_all_sources(df):\n",
    "    \"\"\"Process response_keep flag to ensure all sources for a query are kept or none are.\n",
    "    If any source for a query has response_keep='no', set all sources for that query to 'no'.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): DataFrame containing responses with response_keep flags\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: DataFrame with updated response_keep flags\n",
    "    \"\"\"\n",
    "    # Get unique query IDs\n",
    "    unique_query_ids = df['id'].unique()\n",
    "    \n",
    "    # For each query ID\n",
    "    for query_id in unique_query_ids:\n",
    "        # Get all responses for this query\n",
    "        query_responses = df[df['id'] == query_id]\n",
    "        \n",
    "        # If any response has keep='no', set all to 'no', otherwise set all to 'yes'\n",
    "        if (query_responses['response_keep'] == 'no').any():\n",
    "            df.loc[df['id'] == query_id, 'response_keep_for_all_sources'] = 'no'\n",
    "        else:\n",
    "            df.loc[df['id'] == query_id, 'response_keep_for_all_sources'] = 'yes'\n",
    "            \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and process the dataframe\n",
    "base_df = create_base_dataframe(df)\n",
    "processed_data = process_responses(base_df)\n",
    "processed_data = process_keep_for_all_sources(processed_data)\n",
    "processed_data.to_csv('../processed_data_1000_queries.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of queries where all responses were kept: 803\n",
      "Total number of unique queries: 1000\n",
      "Percentage: 80.30%\n",
      "\n",
      "Query IDs with all responses kept: [0, 1, 2, 3, 6, 7, 8, 9, 12, 13, 14, 15, 16, 18, 19, 20, 21, 22, 23, 24, 25, 28, 29, 30, 32, 33, 35, 38, 39, 40, 41, 42, 43, 44, 45, 46, 49, 50, 52, 55, 57, 58, 59, 60, 61, 62, 63, 66, 68, 69, 71, 72, 73, 76, 77, 79, 80, 81, 83, 84, 85, 86, 87, 88, 90, 91, 93, 94, 95, 96, 97, 99, 100, 101, 102, 103, 105, 108, 109, 110, 111, 113, 114, 116, 119, 121, 124, 125, 127, 128, 129, 131, 133, 138, 139, 141, 142, 144, 145, 146, 147, 148, 149, 151, 152, 153, 154, 155, 156, 157, 161, 162, 163, 164, 167, 168, 169, 170, 171, 172, 176, 177, 178, 179, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 192, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 206, 207, 208, 209, 210, 211, 213, 215, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 229, 231, 232, 233, 234, 236, 237, 239, 240, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 255, 257, 258, 259, 260, 262, 263, 264, 265, 266, 267, 269, 270, 272, 273, 274, 275, 277, 279, 280, 282, 283, 284, 285, 286, 287, 288, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 301, 302, 304, 305, 307, 308, 309, 311, 312, 314, 315, 317, 319, 320, 321, 322, 323, 324, 325, 326, 328, 329, 330, 331, 332, 333, 334, 336, 337, 338, 339, 340, 341, 343, 344, 345, 346, 347, 348, 349, 350, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 364, 365, 366, 367, 368, 369, 370, 371, 372, 374, 375, 377, 378, 379, 380, 381, 382, 385, 386, 387, 388, 389, 390, 392, 393, 395, 396, 398, 399, 401, 402, 403, 404, 405, 406, 407, 408, 410, 411, 412, 414, 415, 416, 417, 418, 419, 420, 421, 422, 425, 426, 427, 428, 429, 430, 431, 433, 434, 435, 436, 438, 439, 440, 441, 442, 443, 444, 445, 447, 448, 449, 450, 451, 452, 453, 454, 455, 457, 458, 459, 460, 461, 462, 463, 464, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 491, 493, 494, 495, 496, 497, 498, 499, 501, 503, 505, 507, 508, 509, 510, 511, 512, 513, 514, 516, 519, 520, 522, 523, 524, 525, 528, 529, 530, 531, 532, 533, 535, 536, 537, 538, 541, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 554, 555, 557, 558, 559, 560, 562, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 576, 577, 578, 579, 580, 581, 582, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 611, 612, 613, 614, 615, 616, 617, 618, 619, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 649, 650, 651, 652, 653, 654, 655, 656, 659, 660, 661, 662, 663, 665, 666, 667, 668, 669, 670, 671, 672, 673, 675, 676, 677, 678, 679, 680, 681, 682, 684, 685, 686, 687, 688, 689, 691, 692, 693, 694, 695, 696, 697, 699, 701, 702, 703, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 749, 750, 752, 753, 754, 755, 756, 757, 758, 759, 761, 763, 764, 766, 767, 770, 771, 772, 773, 774, 776, 777, 778, 780, 781, 782, 785, 786, 788, 789, 790, 792, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 829, 830, 831, 832, 833, 834, 835, 837, 838, 839, 840, 841, 843, 844, 845, 846, 847, 848, 850, 851, 852, 853, 854, 855, 856, 857, 859, 860, 861, 862, 863, 864, 865, 866, 867, 869, 870, 871, 872, 873, 874, 876, 877, 878, 879, 880, 881, 882, 885, 886, 887, 889, 892, 893, 895, 897, 898, 899, 900, 902, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 918, 919, 920, 922, 923, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 938, 939, 940, 941, 943, 944, 945, 947, 948, 949, 951, 952, 953, 954, 955, 957, 958, 960, 961, 965, 967, 968, 969, 970, 971, 972, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 985, 986, 987, 989, 991, 993, 995, 996, 997, 998, 999]\n"
     ]
    }
   ],
   "source": [
    "# Count how many query_ids have all responses kept and save IDs\n",
    "keep_count = 0\n",
    "unique_query_ids = list(processed_data['id'].unique())\n",
    "kept_query_ids = []\n",
    "\n",
    "for query_id in unique_query_ids:\n",
    "    # Get all rows for this query_id\n",
    "    query_rows = processed_data[processed_data['id'] == query_id]\n",
    "    \n",
    "    # Check if all responses for this query are kept\n",
    "    if (query_rows['response_keep_for_all_sources'] == 'yes').all():\n",
    "        keep_count += 1\n",
    "        kept_query_ids.append(int(query_id))\n",
    "\n",
    "print(f\"Number of queries where all responses were kept: {keep_count}\")\n",
    "print(f\"Total number of unique queries: {len(unique_query_ids)}\")\n",
    "print(f\"Percentage: {(keep_count/len(unique_query_ids))*100:.2f}%\")\n",
    "print(f\"\\nQuery IDs with all responses kept: {kept_query_ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. OpenAI Text Processing Functions\n",
    "\n",
    "## Purpose\n",
    "Implements intelligent text processing using OpenAI's GPT-4o to analyze and improve text quality by converting incomplete sentences into complete, standalone sentences. Processes text only when necessary, preserving original meaning without adding new information, and gracefully falls back to the original on API failure. Uses GPT-4o with a temperature of 0 for consistent output, supports up to 16,000 tokens for long inputs, and employs Pydantic models for structured, reliable parsing. Validates output length, retries processing when results are inconsistent, and falls back to the original text if processing fails.\n",
    "\n",
    "The \"sentencified\" data is used in claim comparison.\n",
    "\n",
    "### Sentence Completion Functions:\n",
    "- **`are_all_sentences_complete()`**: Uses OpenAI's structured output to determine if text contains complete sentences\n",
    "- **`complete_sentences()`**: Transforms incomplete text into complete sentences using GPT-4o\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== OPENAI PROCESSING FUNCTIONS =====\n",
    "\n",
    "class SentenceCompleteness(BaseModel):\n",
    "    \"\"\"Structured output indicating whether all sentences are complete.\"\"\"\n",
    "    all_sentences_complete: bool\n",
    "\n",
    "def are_all_sentences_complete(text: str) -> bool:\n",
    "    \"\"\"\n",
    "    Returns True if all sentences in the text are complete; False if any are incomplete.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.responses.parse(\n",
    "            model=\"gpt-4o\",\n",
    "            input=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": (\n",
    "                        \"Analyze the following text. Return true if all sentences are complete; \"\n",
    "                        \"Return false if there are tables, bullet points, or lists in the text\"\n",
    "                        \"return false if any sentence is incomplete. \"\n",
    "                        \"A complete sentence must have a subject, a predicate, and express a complete thought.\"\n",
    "                    ),\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": text,\n",
    "                },\n",
    "            ],\n",
    "            text_format=SentenceCompleteness,\n",
    "            temperature=0,\n",
    "        )\n",
    "        return response.output_parsed.all_sentences_complete\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking sentence completeness: {e}\")\n",
    "        return False\n",
    "    \n",
    "def complete_sentences(text: str) -> str:\n",
    "    \"\"\"Convert incomplete sentences into complete sentences using OpenAI\"\"\"\n",
    "    instruction = \"Convert only incomplete sentences into complete sentences, adding no new information. If a sentence is already complete, do not change it in any way. Each new sentence must include enough context to be fully understood on its own. No headers. Transform all bullet points and tables if they exist into complete sentences. Do not include explanatory text about the transformation process.\"\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": instruction},\n",
    "                {\"role\": \"user\", \"content\": f\"\"\"\n",
    "                    Instruction: {instruction}\n",
    "                    \n",
    "                    Text: {text}\n",
    "                \"\"\"}\n",
    "            ],\n",
    "            temperature=0,\n",
    "            max_tokens=16000\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error completing sentences: {e}\")\n",
    "        return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Turn into Complete Sentences \n",
    "\n",
    "This section transforms responses into complete sentences to enable accurate claim comparison. The results are stored in `response_already_complete_sentences` for sentences that were already complete, and `response_complete_sentences` for the final complete sentences.\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_batch(processed_data, start_idx=0, batch_size=5, verbose=True):\n",
    "    \"\"\"\n",
    "    Process a batch of data to check and complete sentences.\n",
    "    \n",
    "    Args:\n",
    "        processed_data (pd.DataFrame): DataFrame containing the data to process\n",
    "        start_idx (int): Starting index for the batch\n",
    "        batch_size (int): Size of batch to process\n",
    "        verbose (bool): Whether to print progress information\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Processed batch with complete sentences\n",
    "    \"\"\"\n",
    "    # Get batch sample\n",
    "    end_idx = min(start_idx + batch_size, len(processed_data))\n",
    "    data_batch = processed_data.iloc[start_idx:end_idx].copy()\n",
    "    \n",
    "    # Only process rows where response_keep_for_all_sources is 'yes'\n",
    "    data_batch_to_process = data_batch[data_batch['response_keep'] == 'yes'].copy()\n",
    "    \n",
    "    if len(data_batch_to_process) > 0:\n",
    "        # Apply sentence completeness check and transform incomplete sentences\n",
    "        if verbose:\n",
    "            print(f\"\\nProcessing batch from index {start_idx} to {end_idx-1}\")\n",
    "            print(\"\\nInitial sentence completeness check:\")\n",
    "            for idx, row in data_batch_to_process.iterrows():\n",
    "                result = are_all_sentences_complete(row['response_cleaned'])\n",
    "                print(f\"Row {idx}: {result}\")\n",
    "\n",
    "        # Check sentence completeness for all rows\n",
    "        data_batch.loc[data_batch_to_process.index, 'response_already_complete_sentences'] = \\\n",
    "            data_batch_to_process['response_cleaned'].apply(are_all_sentences_complete)\n",
    "\n",
    "        # Initialize response_complete_sentences column with response_no_headers_or_markdown\n",
    "        data_batch.loc[data_batch_to_process.index, 'response_complete_sentences'] = \\\n",
    "            data_batch_to_process['response_no_headers_or_markdown']\n",
    "        # Process rows with incomplete sentences\n",
    "        incomplete_rows = data_batch_to_process[~data_batch_to_process['response_cleaned'].apply(are_all_sentences_complete)]\n",
    "        for idx in incomplete_rows.index:\n",
    "            data_batch.loc[idx, 'response_complete_sentences'] = complete_sentences(incomplete_rows.loc[idx, 'response_cleaned'])\n",
    "\n",
    "        # Verify all sentences are now complete\n",
    "        if verbose:\n",
    "            print(\"\\nFinal sentence completeness check:\")\n",
    "            for idx, row in data_batch_to_process.iterrows():\n",
    "                result = are_all_sentences_complete(row['response_complete_sentences'])\n",
    "                print(f\"Row {idx}: {result}\")\n",
    "\n",
    "            # Print summary\n",
    "            num_transformed = len(incomplete_rows)\n",
    "            num_incomplete = sum(~data_batch_to_process['response_complete_sentences'].apply(are_all_sentences_complete))\n",
    "            print(f\"\\nNumber of responses transformed: {num_transformed}\")\n",
    "            print(f\"Responses with incomplete sentences remaining: {num_incomplete}\")\n",
    "    \n",
    "    return data_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process data in batches and save to cleaned_output folder\n",
    "batch_size = 5\n",
    "total_rows = len(processed_data)\n",
    "\n",
    "for start_idx in range(0, total_rows, batch_size):\n",
    "    # Process batch\n",
    "    batch_df = process_data_batch(processed_data, start_idx=start_idx, batch_size=batch_size, verbose=True)\n",
    "    \n",
    "    # Update processed_data with the processed batch\n",
    "    end_idx = min(start_idx + batch_size, total_rows)\n",
    "    processed_data.iloc[start_idx:end_idx] = batch_df\n",
    "    \n",
    "    # Create filename with batch range\n",
    "    filename = f'/cleaned_output/processed_data_batch_{start_idx}_{end_idx}.csv'\n",
    "    \n",
    "    # Save batch to CSV\n",
    "    batch_df.to_csv(filename, index=False)\n",
    "    print(f\"Saved batch {start_idx}-{end_idx} to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of all CSV files in cleaned_output directory\n",
    "batch_files = glob.glob('cleaned_output/processed_data_batch_*.csv')\n",
    "\n",
    "# Sort files by batch number\n",
    "batch_files.sort(key=lambda x: int(x.split('_')[-2]))\n",
    "\n",
    "# Read and combine all batches\n",
    "combined_df = pd.concat([pd.read_csv(f) for f in batch_files], ignore_index=True)\n",
    "\n",
    "# Save combined dataset\n",
    "combined_df.to_csv('../processed_data_1000_queries.csv', index=False)\n",
    "print(f\"Combined {len(batch_files)} batches into processed_data_combined.csv\")\n",
    "print(f\"Total rows in combined dataset: {len(combined_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Wide Version of Data (Optional)\n",
    "\n",
    "This is a wide-format dataset where each row represents one query, and columns contain responses from multiple sources.Each source’s outputs are stored in separate prefixed columns (e.g., gpt_4o_response, gemini_response_cleaned, USHMM_response_language, ...), enabling direct comparison across models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>original_query</th>\n",
       "      <th>USHMM_response</th>\n",
       "      <th>USHMM_response_cleaned</th>\n",
       "      <th>USHMM_response_no_headers</th>\n",
       "      <th>USHMM_response_no_headers_or_markdown</th>\n",
       "      <th>USHMM_response_language</th>\n",
       "      <th>USHMM_response_refusal</th>\n",
       "      <th>USHMM_response_keep</th>\n",
       "      <th>USHMM_response_keep_for_all_sources</th>\n",
       "      <th>...</th>\n",
       "      <th>grok_response</th>\n",
       "      <th>grok_response_cleaned</th>\n",
       "      <th>grok_response_no_headers</th>\n",
       "      <th>grok_response_no_headers_or_markdown</th>\n",
       "      <th>grok_response_language</th>\n",
       "      <th>grok_response_refusal</th>\n",
       "      <th>grok_response_keep</th>\n",
       "      <th>grok_response_keep_for_all_sources</th>\n",
       "      <th>grok_response_already_complete_sentences</th>\n",
       "      <th>grok_response_complete_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>how many people died in the holocaust</td>\n",
       "      <td>#How Many People did the Nazis Murder? | Holoc...</td>\n",
       "      <td>#How Many People did the Nazis Murder? | Holoc...</td>\n",
       "      <td>\\nNazi Germany committed mass murder on an unp...</td>\n",
       "      <td>Nazi Germany committed mass murder on an unpre...</td>\n",
       "      <td>en</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>...</td>\n",
       "      <td>The Holocaust was a period of systematic perse...</td>\n",
       "      <td>The Holocaust was a period of systematic perse...</td>\n",
       "      <td>The Holocaust was a period of systematic perse...</td>\n",
       "      <td>The Holocaust was a period of systematic perse...</td>\n",
       "      <td>en</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "      <td>The Holocaust was a period of systematic perse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>armenian genocide</td>\n",
       "      <td>#The Armenian Genocide (1915-16): Overview | H...</td>\n",
       "      <td>#The Armenian Genocide (1915-16): Overview | H...</td>\n",
       "      <td>\\nSometimes called the first genocide of the t...</td>\n",
       "      <td>Sometimes called the first genocide of the twe...</td>\n",
       "      <td>en</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>...</td>\n",
       "      <td>The Armenian Genocide was the systematic exter...</td>\n",
       "      <td>The Armenian Genocide was the systematic exter...</td>\n",
       "      <td>The Armenian Genocide was the systematic exter...</td>\n",
       "      <td>The Armenian Genocide was the systematic exter...</td>\n",
       "      <td>en</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "      <td>The Armenian Genocide was the systematic exter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>holocaust encyclopedia</td>\n",
       "      <td>#Introduction to the Holocaust: What was the H...</td>\n",
       "      <td>#Introduction to the Holocaust: What was the H...</td>\n",
       "      <td>\\nThe Holocaust (1933–1945) was the systematic...</td>\n",
       "      <td>The Holocaust (1933–1945) was the systematic, ...</td>\n",
       "      <td>en</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>...</td>\n",
       "      <td>The Holocaust Encyclopedia is a comprehensive ...</td>\n",
       "      <td>The Holocaust Encyclopedia is a comprehensive ...</td>\n",
       "      <td>The Holocaust Encyclopedia is a comprehensive ...</td>\n",
       "      <td>The Holocaust Encyclopedia is a comprehensive ...</td>\n",
       "      <td>en</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "      <td>The Holocaust Encyclopedia is a comprehensive ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>first they came for</td>\n",
       "      <td>#Martin Niemöller: \"First they came for the So...</td>\n",
       "      <td>#Martin Niemöller: \"First they came for the So...</td>\n",
       "      <td>\\n\\n&gt; First they came for the socialists, and ...</td>\n",
       "      <td>First they came for the socialists, and I did ...</td>\n",
       "      <td>en</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>...</td>\n",
       "      <td>\"First they came ...\" is the beginning of a fa...</td>\n",
       "      <td>\"First they came ...\" is the beginning of a fa...</td>\n",
       "      <td>\"First they came ...\" is the beginning of a fa...</td>\n",
       "      <td>\"First they came ...\" is the beginning of a fa...</td>\n",
       "      <td>en</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "      <td>\"First they came ...\" is the beginning of a fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>holocaust</td>\n",
       "      <td>#Introduction to the Holocaust: What was the H...</td>\n",
       "      <td>#Introduction to the Holocaust: What was the H...</td>\n",
       "      <td>\\nThe Holocaust (1933–1945) was the systematic...</td>\n",
       "      <td>The Holocaust (1933–1945) was the systematic, ...</td>\n",
       "      <td>en</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>...</td>\n",
       "      <td>Der Holocaust war eine systematische, staatlic...</td>\n",
       "      <td>Der Holocaust war eine systematische, staatlic...</td>\n",
       "      <td>Der Holocaust war eine systematische, staatlic...</td>\n",
       "      <td>Der Holocaust war eine systematische, staatlic...</td>\n",
       "      <td>de</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                         original_query  \\\n",
       "0  0  how many people died in the holocaust   \n",
       "1  1                      armenian genocide   \n",
       "2  2                 holocaust encyclopedia   \n",
       "3  3                    first they came for   \n",
       "4  4                              holocaust   \n",
       "\n",
       "                                      USHMM_response  \\\n",
       "0  #How Many People did the Nazis Murder? | Holoc...   \n",
       "1  #The Armenian Genocide (1915-16): Overview | H...   \n",
       "2  #Introduction to the Holocaust: What was the H...   \n",
       "3  #Martin Niemöller: \"First they came for the So...   \n",
       "4  #Introduction to the Holocaust: What was the H...   \n",
       "\n",
       "                              USHMM_response_cleaned  \\\n",
       "0  #How Many People did the Nazis Murder? | Holoc...   \n",
       "1  #The Armenian Genocide (1915-16): Overview | H...   \n",
       "2  #Introduction to the Holocaust: What was the H...   \n",
       "3  #Martin Niemöller: \"First they came for the So...   \n",
       "4  #Introduction to the Holocaust: What was the H...   \n",
       "\n",
       "                           USHMM_response_no_headers  \\\n",
       "0  \\nNazi Germany committed mass murder on an unp...   \n",
       "1  \\nSometimes called the first genocide of the t...   \n",
       "2  \\nThe Holocaust (1933–1945) was the systematic...   \n",
       "3  \\n\\n> First they came for the socialists, and ...   \n",
       "4  \\nThe Holocaust (1933–1945) was the systematic...   \n",
       "\n",
       "               USHMM_response_no_headers_or_markdown USHMM_response_language  \\\n",
       "0  Nazi Germany committed mass murder on an unpre...                      en   \n",
       "1  Sometimes called the first genocide of the twe...                      en   \n",
       "2  The Holocaust (1933–1945) was the systematic, ...                      en   \n",
       "3  First they came for the socialists, and I did ...                      en   \n",
       "4  The Holocaust (1933–1945) was the systematic, ...                      en   \n",
       "\n",
       "  USHMM_response_refusal USHMM_response_keep  \\\n",
       "0                     no                 yes   \n",
       "1                     no                 yes   \n",
       "2                     no                 yes   \n",
       "3                     no                 yes   \n",
       "4                     no                 yes   \n",
       "\n",
       "  USHMM_response_keep_for_all_sources  ...  \\\n",
       "0                                 yes  ...   \n",
       "1                                 yes  ...   \n",
       "2                                 yes  ...   \n",
       "3                                 yes  ...   \n",
       "4                                  no  ...   \n",
       "\n",
       "                                       grok_response  \\\n",
       "0  The Holocaust was a period of systematic perse...   \n",
       "1  The Armenian Genocide was the systematic exter...   \n",
       "2  The Holocaust Encyclopedia is a comprehensive ...   \n",
       "3  \"First they came ...\" is the beginning of a fa...   \n",
       "4  Der Holocaust war eine systematische, staatlic...   \n",
       "\n",
       "                               grok_response_cleaned  \\\n",
       "0  The Holocaust was a period of systematic perse...   \n",
       "1  The Armenian Genocide was the systematic exter...   \n",
       "2  The Holocaust Encyclopedia is a comprehensive ...   \n",
       "3  \"First they came ...\" is the beginning of a fa...   \n",
       "4  Der Holocaust war eine systematische, staatlic...   \n",
       "\n",
       "                            grok_response_no_headers  \\\n",
       "0  The Holocaust was a period of systematic perse...   \n",
       "1  The Armenian Genocide was the systematic exter...   \n",
       "2  The Holocaust Encyclopedia is a comprehensive ...   \n",
       "3  \"First they came ...\" is the beginning of a fa...   \n",
       "4  Der Holocaust war eine systematische, staatlic...   \n",
       "\n",
       "                grok_response_no_headers_or_markdown grok_response_language  \\\n",
       "0  The Holocaust was a period of systematic perse...                     en   \n",
       "1  The Armenian Genocide was the systematic exter...                     en   \n",
       "2  The Holocaust Encyclopedia is a comprehensive ...                     en   \n",
       "3  \"First they came ...\" is the beginning of a fa...                     en   \n",
       "4  Der Holocaust war eine systematische, staatlic...                     de   \n",
       "\n",
       "  grok_response_refusal grok_response_keep grok_response_keep_for_all_sources  \\\n",
       "0                    no                yes                                yes   \n",
       "1                    no                yes                                yes   \n",
       "2                    no                yes                                yes   \n",
       "3                    no                yes                                yes   \n",
       "4                    no                 no                                 no   \n",
       "\n",
       "  grok_response_already_complete_sentences  \\\n",
       "0                                     True   \n",
       "1                                    False   \n",
       "2                                    False   \n",
       "3                                     True   \n",
       "4                                      NaN   \n",
       "\n",
       "                    grok_response_complete_sentences  \n",
       "0  The Holocaust was a period of systematic perse...  \n",
       "1  The Armenian Genocide was the systematic exter...  \n",
       "2  The Holocaust Encyclopedia is a comprehensive ...  \n",
       "3  \"First they came ...\" is the beginning of a fa...  \n",
       "4                                                NaN  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_wide_format_data(processed_data):\n",
    "    # Create list of base columns that get repeated for each source\n",
    "    base_columns = list(processed_data.columns[~processed_data.columns.isin(['id', 'source', 'original_query'])])\n",
    "\n",
    "    # Define sources\n",
    "    sources = processed_data['source'].unique()\n",
    "\n",
    "    # Create full column list starting with id and original_query\n",
    "    columns = ['id', 'original_query']\n",
    "\n",
    "    # Add source-specific columns\n",
    "    for source in sources:\n",
    "        source_columns = [f\"{source}_{col}\" for col in base_columns]\n",
    "        columns.extend(source_columns)\n",
    "\n",
    "    # Create empty dataframe with the defined columns\n",
    "    wide_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "    # Copy data from processed_data to wide_df\n",
    "    for idx, row in processed_data.iterrows():\n",
    "        source = row['source']\n",
    "        query_id = row['id']\n",
    "        \n",
    "        # If this query_id doesn't exist in wide_df yet, create it\n",
    "        if query_id not in wide_df['id'].values:\n",
    "            new_row = pd.DataFrame({\n",
    "                'id': [query_id],\n",
    "                'original_query': row['original_query']  # Copy original_query when creating new row\n",
    "            })\n",
    "            wide_df = pd.concat([wide_df, new_row], ignore_index=True)\n",
    "        \n",
    "        # Get the row index in wide_df\n",
    "        wide_idx = wide_df[wide_df['id'] == query_id].index[0]\n",
    "        \n",
    "        # Copy over the data\n",
    "        for base_col in base_columns:\n",
    "            if base_col in row:\n",
    "                wide_col = f\"{source}_{base_col}\"\n",
    "                wide_df.at[wide_idx, wide_col] = row[base_col]\n",
    "\n",
    "    return wide_df\n",
    "\n",
    "# Create wide format data and save to CSV\n",
    "wide_df = create_wide_format_data(processed_data)\n",
    "wide_df.to_csv('wide_data_1000_queries.csv', index=False)\n",
    "wide_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Dataset Helper Functions (Optional)\n",
    "\n",
    "These are functions to help create smaller versions of the datasets that are fed into other analysis/pipelines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the processed data\n",
    "processed_data = pd.read_csv('../processed_data_1000_queries.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created filtered dataset with 1910 rows\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created wide format dataset with 955 rows\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>original_query</th>\n",
       "      <th>USHMM_response</th>\n",
       "      <th>USHMM_response_cleaned</th>\n",
       "      <th>USHMM_response_no_headers</th>\n",
       "      <th>USHMM_response_no_headers_or_markdown</th>\n",
       "      <th>USHMM_response_language</th>\n",
       "      <th>USHMM_response_refusal</th>\n",
       "      <th>USHMM_response_keep</th>\n",
       "      <th>USHMM_response_keep_for_all_sources</th>\n",
       "      <th>...</th>\n",
       "      <th>gpt_4o_response</th>\n",
       "      <th>gpt_4o_response_cleaned</th>\n",
       "      <th>gpt_4o_response_no_headers</th>\n",
       "      <th>gpt_4o_response_no_headers_or_markdown</th>\n",
       "      <th>gpt_4o_response_language</th>\n",
       "      <th>gpt_4o_response_refusal</th>\n",
       "      <th>gpt_4o_response_keep</th>\n",
       "      <th>gpt_4o_response_keep_for_all_sources</th>\n",
       "      <th>gpt_4o_response_already_complete_sentences</th>\n",
       "      <th>gpt_4o_response_complete_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>how many people died in the holocaust</td>\n",
       "      <td>#How Many People did the Nazis Murder? | Holoc...</td>\n",
       "      <td>#How Many People did the Nazis Murder? | Holoc...</td>\n",
       "      <td>\\nNazi Germany committed mass murder on an unp...</td>\n",
       "      <td>Nazi Germany committed mass murder on an unpre...</td>\n",
       "      <td>en</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>...</td>\n",
       "      <td>Approximately 6 million Jews were killed durin...</td>\n",
       "      <td>Approximately 6 million Jews were killed durin...</td>\n",
       "      <td>Approximately 6 million Jews were killed durin...</td>\n",
       "      <td>Approximately 6 million Jews were killed durin...</td>\n",
       "      <td>en</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "      <td>Approximately 6 million Jews were killed durin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>armenian genocide</td>\n",
       "      <td>#The Armenian Genocide (1915-16): Overview | H...</td>\n",
       "      <td>#The Armenian Genocide (1915-16): Overview | H...</td>\n",
       "      <td>\\nSometimes called the first genocide of the t...</td>\n",
       "      <td>Sometimes called the first genocide of the twe...</td>\n",
       "      <td>en</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>...</td>\n",
       "      <td>The Armenian Genocide was the systematic mass ...</td>\n",
       "      <td>The Armenian Genocide was the systematic mass ...</td>\n",
       "      <td>The Armenian Genocide was the systematic mass ...</td>\n",
       "      <td>The Armenian Genocide was the systematic mass ...</td>\n",
       "      <td>en</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "      <td>April 24, 1915, is often marked as the beginni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>holocaust encyclopedia</td>\n",
       "      <td>#Introduction to the Holocaust: What was the H...</td>\n",
       "      <td>#Introduction to the Holocaust: What was the H...</td>\n",
       "      <td>\\nThe Holocaust (1933–1945) was the systematic...</td>\n",
       "      <td>The Holocaust (1933–1945) was the systematic, ...</td>\n",
       "      <td>en</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>...</td>\n",
       "      <td>The Holocaust Encyclopedia is a comprehensive ...</td>\n",
       "      <td>The Holocaust Encyclopedia is a comprehensive ...</td>\n",
       "      <td>The Holocaust Encyclopedia is a comprehensive ...</td>\n",
       "      <td>The Holocaust Encyclopedia is a comprehensive ...</td>\n",
       "      <td>en</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "      <td>The Holocaust Encyclopedia is a comprehensive ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>first they came for</td>\n",
       "      <td>#Martin Niemöller: \"First they came for the So...</td>\n",
       "      <td>#Martin Niemöller: \"First they came for the So...</td>\n",
       "      <td>\\n\\n&gt; First they came for the socialists, and ...</td>\n",
       "      <td>First they came for the socialists, and I did ...</td>\n",
       "      <td>en</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>...</td>\n",
       "      <td>The phrase \"First they came for...\" is the ope...</td>\n",
       "      <td>The phrase \"First they came for...\" is the ope...</td>\n",
       "      <td>The phrase \"First they came for...\" is the ope...</td>\n",
       "      <td>The phrase \"First they came for...\" is the ope...</td>\n",
       "      <td>en</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "      <td>The phrase \"First they came for...\" is the ope...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>holocaust</td>\n",
       "      <td>#Introduction to the Holocaust: What was the H...</td>\n",
       "      <td>#Introduction to the Holocaust: What was the H...</td>\n",
       "      <td>\\nThe Holocaust (1933–1945) was the systematic...</td>\n",
       "      <td>The Holocaust (1933–1945) was the systematic, ...</td>\n",
       "      <td>en</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>...</td>\n",
       "      <td>The Holocaust was the systematic, state-sponso...</td>\n",
       "      <td>The Holocaust was the systematic, state-sponso...</td>\n",
       "      <td>The Holocaust was the systematic, state-sponso...</td>\n",
       "      <td>The Holocaust was the systematic, state-sponso...</td>\n",
       "      <td>en</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>True</td>\n",
       "      <td>The Holocaust was the systematic, state-sponso...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                         original_query  \\\n",
       "0  0  how many people died in the holocaust   \n",
       "1  1                      armenian genocide   \n",
       "2  2                 holocaust encyclopedia   \n",
       "3  3                    first they came for   \n",
       "4  4                              holocaust   \n",
       "\n",
       "                                      USHMM_response  \\\n",
       "0  #How Many People did the Nazis Murder? | Holoc...   \n",
       "1  #The Armenian Genocide (1915-16): Overview | H...   \n",
       "2  #Introduction to the Holocaust: What was the H...   \n",
       "3  #Martin Niemöller: \"First they came for the So...   \n",
       "4  #Introduction to the Holocaust: What was the H...   \n",
       "\n",
       "                              USHMM_response_cleaned  \\\n",
       "0  #How Many People did the Nazis Murder? | Holoc...   \n",
       "1  #The Armenian Genocide (1915-16): Overview | H...   \n",
       "2  #Introduction to the Holocaust: What was the H...   \n",
       "3  #Martin Niemöller: \"First they came for the So...   \n",
       "4  #Introduction to the Holocaust: What was the H...   \n",
       "\n",
       "                           USHMM_response_no_headers  \\\n",
       "0  \\nNazi Germany committed mass murder on an unp...   \n",
       "1  \\nSometimes called the first genocide of the t...   \n",
       "2  \\nThe Holocaust (1933–1945) was the systematic...   \n",
       "3  \\n\\n> First they came for the socialists, and ...   \n",
       "4  \\nThe Holocaust (1933–1945) was the systematic...   \n",
       "\n",
       "               USHMM_response_no_headers_or_markdown USHMM_response_language  \\\n",
       "0  Nazi Germany committed mass murder on an unpre...                      en   \n",
       "1  Sometimes called the first genocide of the twe...                      en   \n",
       "2  The Holocaust (1933–1945) was the systematic, ...                      en   \n",
       "3  First they came for the socialists, and I did ...                      en   \n",
       "4  The Holocaust (1933–1945) was the systematic, ...                      en   \n",
       "\n",
       "  USHMM_response_refusal USHMM_response_keep  \\\n",
       "0                     no                 yes   \n",
       "1                     no                 yes   \n",
       "2                     no                 yes   \n",
       "3                     no                 yes   \n",
       "4                     no                 yes   \n",
       "\n",
       "  USHMM_response_keep_for_all_sources  ...  \\\n",
       "0                                 yes  ...   \n",
       "1                                 yes  ...   \n",
       "2                                 yes  ...   \n",
       "3                                 yes  ...   \n",
       "4                                  no  ...   \n",
       "\n",
       "                                     gpt_4o_response  \\\n",
       "0  Approximately 6 million Jews were killed durin...   \n",
       "1  The Armenian Genocide was the systematic mass ...   \n",
       "2  The Holocaust Encyclopedia is a comprehensive ...   \n",
       "3  The phrase \"First they came for...\" is the ope...   \n",
       "4  The Holocaust was the systematic, state-sponso...   \n",
       "\n",
       "                             gpt_4o_response_cleaned  \\\n",
       "0  Approximately 6 million Jews were killed durin...   \n",
       "1  The Armenian Genocide was the systematic mass ...   \n",
       "2  The Holocaust Encyclopedia is a comprehensive ...   \n",
       "3  The phrase \"First they came for...\" is the ope...   \n",
       "4  The Holocaust was the systematic, state-sponso...   \n",
       "\n",
       "                          gpt_4o_response_no_headers  \\\n",
       "0  Approximately 6 million Jews were killed durin...   \n",
       "1  The Armenian Genocide was the systematic mass ...   \n",
       "2  The Holocaust Encyclopedia is a comprehensive ...   \n",
       "3  The phrase \"First they came for...\" is the ope...   \n",
       "4  The Holocaust was the systematic, state-sponso...   \n",
       "\n",
       "              gpt_4o_response_no_headers_or_markdown gpt_4o_response_language  \\\n",
       "0  Approximately 6 million Jews were killed durin...                       en   \n",
       "1  The Armenian Genocide was the systematic mass ...                       en   \n",
       "2  The Holocaust Encyclopedia is a comprehensive ...                       en   \n",
       "3  The phrase \"First they came for...\" is the ope...                       en   \n",
       "4  The Holocaust was the systematic, state-sponso...                       en   \n",
       "\n",
       "  gpt_4o_response_refusal gpt_4o_response_keep  \\\n",
       "0                      no                  yes   \n",
       "1                      no                  yes   \n",
       "2                      no                  yes   \n",
       "3                      no                  yes   \n",
       "4                      no                  yes   \n",
       "\n",
       "  gpt_4o_response_keep_for_all_sources  \\\n",
       "0                                  yes   \n",
       "1                                  yes   \n",
       "2                                  yes   \n",
       "3                                  yes   \n",
       "4                                   no   \n",
       "\n",
       "  gpt_4o_response_already_complete_sentences  \\\n",
       "0                                      False   \n",
       "1                                      False   \n",
       "2                                      False   \n",
       "3                                      False   \n",
       "4                                       True   \n",
       "\n",
       "                  gpt_4o_response_complete_sentences  \n",
       "0  Approximately 6 million Jews were killed durin...  \n",
       "1  April 24, 1915, is often marked as the beginni...  \n",
       "2  The Holocaust Encyclopedia is a comprehensive ...  \n",
       "3  The phrase \"First they came for...\" is the ope...  \n",
       "4  The Holocaust was the systematic, state-sponso...  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dataset with only USHMM and GPT data where response_keep is 'yes'\n",
    "def create_ushmm_gpt_dataset(df):\n",
    "    # Filter to keep only rows where response_keep is 'yes'\n",
    "    ushmm_gpt_filtered = df[df['response_keep'] == 'yes']\n",
    "    \n",
    "    # Filter to only USHMM and GPT sources\n",
    "    ushmm_gpt_data = ushmm_gpt_filtered[ushmm_gpt_filtered['source'].isin(['USHMM', 'gpt_4o'])]\n",
    "\n",
    "    # Group by id and check that both USHMM and gpt_4o exist\n",
    "    grouped = ushmm_gpt_data.groupby('id')['source'].apply(set)\n",
    "    valid_ids = grouped[grouped.apply(lambda x: {'USHMM', 'gpt_4o'}.issubset(x))].index\n",
    "    \n",
    "    # Filter to only keep rows where both sources exist\n",
    "    ushmm_gpt_filtered = ushmm_gpt_data[ushmm_gpt_data['id'].isin(valid_ids)]\n",
    "    \n",
    "    return ushmm_gpt_filtered\n",
    "\n",
    "# Create the filtered dataset\n",
    "ushmm_gpt_df = create_ushmm_gpt_dataset(processed_data)\n",
    "print(f\"Created filtered dataset with {len(ushmm_gpt_df)} rows\")\n",
    "\n",
    "# Create wide format of USHMM and GPT data\n",
    "ushmm_gpt_wide = create_wide_format_data(ushmm_gpt_df)\n",
    "print(f\"Created wide format dataset with {len(ushmm_gpt_wide)} rows\")\n",
    "\n",
    "ushmm_gpt_wide.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created filtered dataset with 3212 rows\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>original_query</th>\n",
       "      <th>source</th>\n",
       "      <th>response</th>\n",
       "      <th>response_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>how many people died in the holocaust</td>\n",
       "      <td>USHMM</td>\n",
       "      <td>#How Many People did the Nazis Murder? | Holoc...</td>\n",
       "      <td>#How Many People did the Nazis Murder? | Holoc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>how many people died in the holocaust</td>\n",
       "      <td>gpt_4o</td>\n",
       "      <td>Approximately 6 million Jews were killed durin...</td>\n",
       "      <td>Approximately 6 million Jews were killed durin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>how many people died in the holocaust</td>\n",
       "      <td>gemini</td>\n",
       "      <td>Historians estimate that the Nazis murdered ap...</td>\n",
       "      <td>Historians estimate that the Nazis murdered ap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>how many people died in the holocaust</td>\n",
       "      <td>grok</td>\n",
       "      <td>The Holocaust was a period of systematic perse...</td>\n",
       "      <td>The Holocaust was a period of systematic perse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>armenian genocide</td>\n",
       "      <td>USHMM</td>\n",
       "      <td>#The Armenian Genocide (1915-16): Overview | H...</td>\n",
       "      <td>#The Armenian Genocide (1915-16): Overview | H...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                         original_query  source  \\\n",
       "0   0  how many people died in the holocaust   USHMM   \n",
       "1   0  how many people died in the holocaust  gpt_4o   \n",
       "2   0  how many people died in the holocaust  gemini   \n",
       "3   0  how many people died in the holocaust    grok   \n",
       "4   1                      armenian genocide   USHMM   \n",
       "\n",
       "                                            response  \\\n",
       "0  #How Many People did the Nazis Murder? | Holoc...   \n",
       "1  Approximately 6 million Jews were killed durin...   \n",
       "2  Historians estimate that the Nazis murdered ap...   \n",
       "3  The Holocaust was a period of systematic perse...   \n",
       "4  #The Armenian Genocide (1915-16): Overview | H...   \n",
       "\n",
       "                                    response_cleaned  \n",
       "0  #How Many People did the Nazis Murder? | Holoc...  \n",
       "1  Approximately 6 million Jews were killed durin...  \n",
       "2  Historians estimate that the Nazis murdered ap...  \n",
       "3  The Holocaust was a period of systematic perse...  \n",
       "4  #The Armenian Genocide (1915-16): Overview | H...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the processed data\n",
    "processed_data = pd.read_csv('../processed_data_1000_queries.csv')\n",
    "\n",
    "def create_filtered_dataset(df):\n",
    "    # Filter to keep only rows where response_keep_for_all_sources is 'yes'\n",
    "    filtered_data = df[df['response_keep_for_all_sources'] == 'yes']\n",
    "    # Select only the specified columns\n",
    "    filtered_data = filtered_data[['id', 'original_query', 'source', 'response', 'response_cleaned']]\n",
    "    return filtered_data\n",
    "\n",
    "# Create the filtered dataset\n",
    "filtered_df = create_filtered_dataset(processed_data)\n",
    "print(f\"Created filtered dataset with {len(filtered_df)} rows\")\n",
    "\n",
    "filtered_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
